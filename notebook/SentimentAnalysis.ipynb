{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727020aa-08ca-40b2-a048-8bbf28352ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from   nltk.stem   import WordNetLemmatizer\n",
    "from   nltk.corpus import stopwords\n",
    "\n",
    "from   sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from   sklearn.ensemble        import RandomForestClassifier\n",
    "from   sklearn.tree            import DecisionTreeClassifier\n",
    "from   sklearn.neural_network  import MLPClassifier\n",
    "\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from   sklearn.metrics import classification_report, accuracy_score, hamming_loss, multilabel_confusion_matrix\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6c4c5-e005-4416-a2fb-40e3ce7fc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows    = None\n",
    "\n",
    "\n",
    "TRAIN_SIZE       = 0.95\n",
    "\n",
    "VEC_NGRAM_RANGE  = (1, 3)\n",
    "VEC_SMOOTH_IDF   = True\n",
    "VEC_SUBLINEAR_TF = True\n",
    "\n",
    "\n",
    "TRAIN_N_RFTREES  = 20\n",
    "\n",
    "\n",
    "TRAIN_DECI_TREE_CRITERION = \"gini\"\n",
    "\n",
    "\n",
    "TRAIN_MLPC_SOLVER = \"adam\"\n",
    "TRAIN_MLPC_EPOCHS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ea089-a1fd-43ec-ad3e-5fc441622149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \"GoEmotions\" : https://github.com/google-research/google-research/tree/master/goemotions/data/full_dataset\n",
    "# https://arxiv.org/pdf/2005.00547.pdf\n",
    "\n",
    "df1 = pd.read_csv(\"../../datasets/goemotions_1.csv\")\n",
    "df2 = pd.read_csv(\"../../datasets/goemotions_2.csv\")\n",
    "df3 = pd.read_csv(\"../../datasets/goemotions_3.csv\")\n",
    "\n",
    "# merge all the three segments into one pd.DataFrame\n",
    "df  = pd.concat([df1, df2, df3])\n",
    "\n",
    "print(\"shape: \", df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcbd27-a265-4de4-9d0e-4c9ab970d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up dataset\n",
    "\n",
    "df = df.drop(columns=[\"id\", \"author\", \"subreddit\", \"link_id\", \"parent_id\", \"created_utc\", \"rater_id\"])  # only account for useful columns\n",
    "df = df[df[\"example_very_unclear\"] == False]    # ignore \"unclear\" rows\n",
    "df = df.drop(columns=[\"example_very_unclear\"])  # drop this column\n",
    "\n",
    "\n",
    "print(\"shape: \", df.shape)\n",
    "print(\"null values: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cb315-8d8b-46d4-b544-11b070818072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    res = str()\n",
    "\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    # regexes\n",
    "    url = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    usr_name = r\"@[^\\s]+\"\n",
    "    non_numalpha = r\"[^a-zA-Z0-9]\"\n",
    "\n",
    "    # apply them\n",
    "    text = re.sub(url, \" URL\", text)        # remove urls\n",
    "    text = re.sub(usr_name, \" USER\", text)  # remove user tags\n",
    "    text = re.sub(non_numalpha, \" \", text)  # remove special chars\n",
    "\n",
    "    \n",
    "    # remove stopwords and lemmatize  the rest\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            res += (token + ' ')\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.text = df.text.apply(lambda x : preprocess(x))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dfaeb-dfbc-483d-a16e-c3a0fe85c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels to txt file for frontend use\n",
    "\n",
    "np.savetxt(\"../model/emotions.csv\", [df.columns[1:]], fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513b4e4-03de-4e39-b666-ab5ac23c0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training data (95%) and testing data (5%)\n",
    "\n",
    "X_train_text, X_test_text, y_train_target, y_test_target = train_test_split(df[\"text\"], df[df.columns[1:]], test_size=1-TRAIN_SIZE, random_state=0)\n",
    "\n",
    "\n",
    "print(\"Dataset Size:       \", len(df))\n",
    "print(\"Training Data Size: \", len(X_train_text))\n",
    "print(\"Testing Data Size:  \", len(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9bb49-28f2-4301-9b41-fc5ef30b5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "vectorizer = TfidfVectorizer(ngram_range=VEC_NGRAM_RANGE, smooth_idf=VEC_SMOOTH_IDF, sublinear_tf=VEC_SUBLINEAR_TF)\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "\n",
    "# transform\n",
    "X_train_text = vectorizer.transform(X_train_text)\n",
    "X_test_text  = vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa813f-d32e-41b2-bf51-c0a19a109393",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_accepted = vectorizer.get_feature_names_out()\n",
    "ngrams_ignored  = vectorizer.stop_words_\n",
    "\n",
    "print(\"Number of ngrams accepted (sizes: 1-3): \", len(ngrams_accepted))\n",
    "print(\"Number of ngrams ignored  (sizes: 1-3): \", len(ngrams_ignored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4dbd71-d14c-4989-b854-e2ae11a67752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vectorizer to disk\n",
    "\n",
    "f = open(\"../model/vectorizer.pickle\", \"wb\")\n",
    "pickle.dump(vectorizer, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b9b61-510f-4339-b32a-8fd7d2fff1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_model(model):\n",
    "    y_pred = model.predict(X_test_text)\n",
    "    \n",
    "    print(classification_report(y_test_target, y_pred, zero_division=0))\n",
    "    print(\"Hamming Loss: \", round(hamming_loss(y_test_target, y_pred), 4))\n",
    "    \n",
    "\n",
    "    labels = df.columns[1:]\n",
    "    mlcm = multilabel_confusion_matrix(y_test_target, y_pred)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=7, ncols=4)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.9)\n",
    "    fig.set_figheight(16)\n",
    "    fig.set_figwidth(16)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        group_names = [\"True Neg.\", \"False Pos.\", \"False Neg.\", \"True Pos.\"]\n",
    "        group_percs = [\"{0:.2%}\".format(value) for value in mlcm[i].flatten() / np.sum(mlcm[i])]\n",
    "        annots = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_percs)]\n",
    "        annots = np.asarray(annots).reshape(2, 2)\n",
    "        \n",
    "        sns.heatmap(data=mlcm[i], annot=annots, fmt=\"\", cmap=\"Blues\", cbar=False, ax=axs.flat[i])\n",
    "        \n",
    "        axs.flat[i].set_xlabel(\"Predicted\")\n",
    "        axs.flat[i].set_ylabel(\"Actual\")\n",
    "        axs.flat[i].title.set_text(labels[i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81aa19-69f3-4521-b091-b17e054daa86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN: Random Forest Classifier\n",
    "\n",
    "RFCModel = RandomForestClassifier(verbose=100, n_jobs=-1, n_estimators=TRAIN_N_RFTREES)\n",
    "RFCModel.fit(X_train_text, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244d949-093e-44a3-8b63-4ac60f82be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine Random Forest Classifier Model\n",
    "\n",
    "print(\"Random Forest Classifier Model\")\n",
    "examine_model(RFCModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d9d7a-e874-4651-92c6-a5caaa098dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the Random Forest Classifier Model to disk\n",
    "\n",
    "f = open(\"../model/RFCModel.pickle\", \"wb\")\n",
    "pickle.dump(RFCModel, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb43fbb-0339-47f6-83c0-f672d7e9ab74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN: Decision Tree Classifier\n",
    "\n",
    "DTCModel = DecisionTreeClassifier(criterion=TRAIN_DECI_TREE_CRITERION)\n",
    "DTCModel.fit(X_train_text, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a6d1e-c676-4497-b249-67804004fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine Decision Tree Classifier Model\n",
    "\n",
    "print(\"Decision Tree Classifier Model\")\n",
    "examine_model(DTCModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf783924-55e6-44b8-8c9f-698aad45f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the Decision Tree Classifier Model to disk\n",
    "\n",
    "f = open(\"../model/DTCModel.pickle\", \"wb\")\n",
    "pickle.dump(DTCModel, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac48ea-5607-4c81-8f05-ac3239624f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN: Neural Network - Multi-layer Perceptron Classifier\n",
    "\n",
    "MLPCModel = MLPClassifier(solver=TRAIN_MLPC_SOLVER, max_iter=TRAIN_MLPC_EPOCHS, verbose=100)\n",
    "MLPCModel.fit(X_train_text, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba44ce-15f7-454b-b12c-30d2da038cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine MLP Classifier Model\n",
    "\n",
    "print(\"Neural Network: MLP Classifier Model\")\n",
    "examine_model(MLPCModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf8ee5-c079-4c12-b2e3-4f347fa2de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the MLP Classifier Model to disk\n",
    "\n",
    "f = open(\"../model/MLPCModel.pickle\", \"wb\")\n",
    "pickle.dump(MLPCModel, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dce8a-3b98-438e-af6a-2aa932b56206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_output_df(query_strs, model):\n",
    "    X = vectorizer.transform(list(map(lambda x : preprocess(x), query_strs)))\n",
    "    y = model.predict_proba(X)\n",
    "\n",
    "    labels  = df.columns[1:]\n",
    "\n",
    "    output_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # for all input strings\n",
    "    for i in range(len(query_strs)):\n",
    "        output_df.loc[i, df.columns[0]] = query_strs[i]    # record input text\n",
    "\n",
    "        # for all labels\n",
    "        for j in range(len(y)):\n",
    "            output_df.loc[i, df.columns[j + 1]] = round(y[j][i][1] * 100, 2)    # record label probability\n",
    "\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vizualize_output_df(output_df):\n",
    "    fig, axs = plt.subplots(nrows=output_df.shape[0], ncols=1)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    fig.set_figheight(8 * output_df.shape[0])\n",
    "    fig.set_figwidth(8)\n",
    "\n",
    "    \n",
    "    for i in range(output_df.shape[0]):\n",
    "        axis = axs.flat[i]\n",
    "        \n",
    "        p = sns.barplot(data=output_df.loc[i, df.columns[1:]], ax=axis, color=\"dodgerblue\")\n",
    "        p.bar_label(p.containers[0], fmt=(lambda x: f\"{round(x)}%\" if x > 0 else \"\"), fontsize=6)\n",
    "        \n",
    "        axis.set_xlabel(\"LABELS\")\n",
    "        axis.set_ylabel(\"PROBABILITY (%)\")\n",
    "        axis.tick_params(axis='x', labelrotation=90)\n",
    "        axis.set(ylim=(0, 100))\n",
    "        axis.title.set_text(\"string: \\\"\" + output_df.loc[i, df.columns[0]] + \"\\\"\\n\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38360a3a-c4fb-44ea-b212-b8dae80feb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this stage, our model is ready. let's run some queries\n",
    "\n",
    "query_strs = [\"This cat looks very cute!\", \"I hate YouTube's ad. revenue model\"]\n",
    "model      = RFCModel\n",
    "\n",
    "output_df  = gen_output_df(query_strs, model)\n",
    "\n",
    "vizualize_output_df(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20dd7a5-9582-4081-a0c6-fe0724a7a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(query_strs, model):\n",
    "    texts   = vectorizer.transform(list(map(lambda x : preprocess(x), query_strs)))\n",
    "    targets = model.predict(texts)\n",
    "\n",
    "    \n",
    "    for i in range(len(query_strs)):\n",
    "        text   = query_strs[i]\n",
    "        target = targets[i]\n",
    "        emotions = []\n",
    "    \n",
    "        print(text, end=\" ===> \")\n",
    "    \n",
    "        for j in range(len(target)):\n",
    "            if target[j] == 1:\n",
    "                emotions.append(df.columns[1:][j])\n",
    "    \n",
    "        print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4b876-1b38-4465-a9ec-d77f77f4574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels(query_strs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5eaa5-eb12-4f2b-9aca-d35c07a53989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
